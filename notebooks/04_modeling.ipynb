{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b5b0284",
   "metadata": {},
   "source": [
    "# Phase 4: Modeling Pipeline\n",
    "\n",
    "Train and evaluate machine learning models for smoking cessation prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0acb1",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd548732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import our modules\n",
    "from src.modeling import (\n",
    "    split_data_by_person,\n",
    "    train_logistic_regression,\n",
    "    train_random_forest,\n",
    "    train_xgboost\n",
    ")\n",
    "from src.evaluation import (\n",
    "    evaluate_model,\n",
    "    print_evaluation_report,\n",
    "    plot_roc_curve,\n",
    "    plot_precision_recall_curve,\n",
    "    plot_confusion_matrix\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úì Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9989d2d",
   "metadata": {},
   "source": [
    "## 2. Load Engineered Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cce60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the engineered sample dataset from Phase 3\n",
    "data_path = Path('../data/processed/engineered_phase3_sample.parquet')\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "print(f\"Loaded dataset: {df.shape[0]} rows √ó {df.shape[1]} features\")\n",
    "print(f\"\\nFeatures: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150c9b6",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Target Variable\n",
    "\n",
    "Since we're working with a sample and don't have actual quit outcomes yet, let's create a synthetic target for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic quit_success outcome based on features\n",
    "# Higher success probability for: lower dependence, used methods, higher motivation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Base probability\n",
    "base_prob = 0.3\n",
    "\n",
    "# Adjust based on features\n",
    "prob = np.full(len(df), base_prob)\n",
    "\n",
    "# Lower dependence increases success\n",
    "if 'high_dependence' in df.columns:\n",
    "    prob = np.where(df['high_dependence'] == 0, prob + 0.15, prob - 0.1)\n",
    "\n",
    "# Using any method increases success\n",
    "if 'used_any_method' in df.columns:\n",
    "    prob = np.where(df['used_any_method'] == 1, prob + 0.2, prob)\n",
    "\n",
    "# Clip probabilities\n",
    "prob = np.clip(prob, 0.05, 0.95)\n",
    "\n",
    "# Generate binary outcomes\n",
    "df['quit_success'] = np.random.binomial(1, prob)\n",
    "\n",
    "# Add person_id for proper splitting\n",
    "df['person_id'] = range(len(df))\n",
    "\n",
    "print(f\"Created synthetic target variable\")\n",
    "print(f\"Quit success rate: {df['quit_success'].mean():.3f}\")\n",
    "print(f\"\\nClass balance:\")\n",
    "print(df['quit_success'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11450f6",
   "metadata": {},
   "source": [
    "## 4. Prepare Feature List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all feature columns (exclude target and ID)\n",
    "exclude_cols = ['quit_success', 'person_id', 'race_ethnicity']  # race_ethnicity is string, use dummies instead\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"Using {len(feature_cols)} features for modeling:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0cc54",
   "metadata": {},
   "source": [
    "## 5. Train/Validation/Test Split\n",
    "\n",
    "Split by person to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187341fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 60% train, 20% val, 20% test\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, train_ids, val_ids, test_ids = split_data_by_person(\n",
    "    df,\n",
    "    feature_cols=feature_cols,\n",
    "    test_size=0.4,\n",
    "    val_size=0.5,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f3f2bf",
   "metadata": {},
   "source": [
    "## 6. Train Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c26b4b",
   "metadata": {},
   "source": [
    "### 6.1 Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb3074d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Logistic Regression...\")\n",
    "lr_model, lr_scaler, y_val_pred_lr, y_val_proba_lr = train_logistic_regression(\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n",
    "print(\"‚úì Logistic Regression trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e90683a",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Random Forest...\")\n",
    "rf_model, y_val_pred_rf, y_val_proba_rf = train_random_forest(\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n",
    "print(\"‚úì Random Forest trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0457a43",
   "metadata": {},
   "source": [
    "### 6.3 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0eaa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost...\")\n",
    "xgb_model, y_val_pred_xgb, y_val_proba_xgb = train_xgboost(\n",
    "    X_train, y_train, X_val, y_val\n",
    ")\n",
    "print(\"‚úì XGBoost trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c30e6f",
   "metadata": {},
   "source": [
    "## 7. Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6baa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "lr_metrics = evaluate_model(y_val, y_val_pred_lr, y_val_proba_lr, \"Logistic Regression\")\n",
    "rf_metrics = evaluate_model(y_val, y_val_pred_rf, y_val_proba_rf, \"Random Forest\")\n",
    "xgb_metrics = evaluate_model(y_val, y_val_pred_xgb, y_val_proba_xgb, \"XGBoost\")\n",
    "\n",
    "# Print reports\n",
    "print_evaluation_report(lr_metrics)\n",
    "print(\"\\n\")\n",
    "print_evaluation_report(rf_metrics)\n",
    "print(\"\\n\")\n",
    "print_evaluation_report(xgb_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d8f490",
   "metadata": {},
   "source": [
    "## 8. Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame([\n",
    "    lr_metrics,\n",
    "    rf_metrics,\n",
    "    xgb_metrics\n",
    "])\n",
    "\n",
    "# Select key metrics\n",
    "display_cols = ['model', 'roc_auc', 'pr_auc', 'precision', 'recall', 'f1']\n",
    "comparison_table = comparison_df[display_cols].round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_table.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_idx = comparison_df['roc_auc'].idxmax()\n",
    "best_model_name = comparison_df.loc[best_model_idx, 'model']\n",
    "print(f\"\\nüèÜ Best model (by ROC-AUC): {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a23640",
   "metadata": {},
   "source": [
    "## 9. Visualization: ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ad6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "plot_roc_curve(y_val, y_val_proba_lr, \"Logistic Regression\", ax=ax)\n",
    "plot_roc_curve(y_val, y_val_proba_rf, \"Random Forest\", ax=ax)\n",
    "plot_roc_curve(y_val, y_val_proba_xgb, \"XGBoost\", ax=ax)\n",
    "\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d34d9",
   "metadata": {},
   "source": [
    "## 10. Visualization: Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "plot_precision_recall_curve(y_val, y_val_proba_lr, \"Logistic Regression\", ax=ax)\n",
    "plot_precision_recall_curve(y_val, y_val_proba_rf, \"Random Forest\", ax=ax)\n",
    "plot_precision_recall_curve(y_val, y_val_proba_xgb, \"XGBoost\", ax=ax)\n",
    "\n",
    "plt.title('Precision-Recall Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77afcc",
   "metadata": {},
   "source": [
    "## 11. Feature Importance (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from XGBoost\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "top_n = min(20, len(importance_df))\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "sns.barplot(\n",
    "    data=importance_df.head(top_n),\n",
    "    x='importance',\n",
    "    y='feature',\n",
    "    palette='viridis',\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_title(f'Top {top_n} Feature Importances (XGBoost)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance Score')\n",
    "ax.set_ylabel('Feature')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 10 Most Important Features:\")\n",
    "for idx, row in importance_df.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:30s} {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db853ff5",
   "metadata": {},
   "source": [
    "## 12. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924d9636",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "plot_confusion_matrix(y_val, y_val_pred_lr, \"Logistic Regression\", ax=axes[0])\n",
    "plot_confusion_matrix(y_val, y_val_pred_rf, \"Random Forest\", ax=axes[1])\n",
    "plot_confusion_matrix(y_val, y_val_pred_xgb, \"XGBoost\", ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ecb29",
   "metadata": {},
   "source": [
    "## 13. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.modeling import save_model\n",
    "\n",
    "# Save the best performing model\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Determine best model\n",
    "best_metrics = comparison_df.loc[best_model_idx]\n",
    "if best_model_name == 'Logistic Regression':\n",
    "    best_model = lr_model\n",
    "    model_path = model_dir / 'logistic_regression_best.pkl'\n",
    "    # Also save scaler\n",
    "    save_model(lr_scaler, model_dir / 'logistic_regression_scaler.pkl')\n",
    "elif best_model_name == 'Random Forest':\n",
    "    best_model = rf_model\n",
    "    model_path = model_dir / 'random_forest_best.pkl'\n",
    "else:\n",
    "    best_model = xgb_model\n",
    "    model_path = model_dir / 'xgboost_best.pkl'\n",
    "\n",
    "# Save model with metadata\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'feature_cols': feature_cols,\n",
    "    'metrics': best_metrics.to_dict(),\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val)\n",
    "}\n",
    "\n",
    "save_model(best_model, model_path, metadata)\n",
    "print(f\"\\n‚úì Best model saved: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46641503",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. ‚úÖ Loaded engineered features from Phase 3\n",
    "2. ‚úÖ Created proper train/validation/test splits (60/20/20)\n",
    "3. ‚úÖ Trained three models:\n",
    "   - Logistic Regression (baseline with feature scaling)\n",
    "   - Random Forest (ensemble method)\n",
    "   - XGBoost (gradient boosting)\n",
    "4. ‚úÖ Evaluated with comprehensive metrics (ROC-AUC, PR-AUC, Precision, Recall, F1)\n",
    "5. ‚úÖ Visualized performance with ROC and PR curves\n",
    "6. ‚úÖ Analyzed feature importance\n",
    "7. ‚úÖ Saved best performing model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Process Full Dataset**: Move beyond 100-row sample to full PATH data\n",
    "2. **Hyperparameter Tuning**: Use GridSearch or RandomSearch for optimization\n",
    "3. **Cross-Validation**: Implement k-fold CV for more robust evaluation\n",
    "4. **Additional Features**: Search for quit history, motivation, environmental variables\n",
    "5. **Final Evaluation**: Test best model on held-out test set\n",
    "6. **Interpretability**: Add SHAP values or other explainability methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
