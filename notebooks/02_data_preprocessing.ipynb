{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eabdda0",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing & Longitudinal Transitions\n",
    "\n",
    "Process full PATH Study data (Waves 1-5) to create person-period dataset with quit outcomes.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Load all 5 waves of PATH Adult Public Use Files\n",
    "2. Create longitudinal person-period structure\n",
    "3. Define quit outcome: smoking status at wave t+1\n",
    "4. Filter to baseline smokers with follow-up data\n",
    "5. Apply feature engineering from Phase 3\n",
    "6. Save `pooled_transitions.csv` for modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee533b25",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c480705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import feature engineering\n",
    "from src.feature_engineering import engineer_all_features, map_from_codebook\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47eafa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for PATH data files:\n",
      "  ✓ Wave 1: PATH_W1_Adult_Public.dta\n",
      "  ✓ Wave 2: PATH_W2_Adult_Public.dta\n",
      "  ✓ Wave 3: PATH_W3_Adult_Public.dta\n",
      "  ✓ Wave 4: PATH_W4_Adult_Public.dta\n",
      "  ✓ Wave 5: PATH_W5_Adult_Public.dta\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path('../data/raw')\n",
    "OUTPUT_DIR = Path('../data/processed')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Wave files\n",
    "WAVE_FILES = {\n",
    "    1: DATA_DIR / 'PATH_W1_Adult_Public.dta',\n",
    "    2: DATA_DIR / 'PATH_W2_Adult_Public.dta',\n",
    "    3: DATA_DIR / 'PATH_W3_Adult_Public.dta',\n",
    "    4: DATA_DIR / 'PATH_W4_Adult_Public.dta',\n",
    "    5: DATA_DIR / 'PATH_W5_Adult_Public.dta',\n",
    "}\n",
    "\n",
    "# Check which files exist\n",
    "print(\"Checking for PATH data files:\")\n",
    "for wave, path in WAVE_FILES.items():\n",
    "    status = \"✓\" if path.exists() else \"✗\"\n",
    "    print(f\"  {status} Wave {wave}: {path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d492cf",
   "metadata": {},
   "source": [
    "## 2. Load Individual Waves\n",
    "\n",
    "Load each wave and extract key smoking status variables for transition analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8426e454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables to track smoking status transitions:\n",
      "  - R0{wave}R_A_EVERSMOKE\n",
      "  - R0{wave}_AC1002\n",
      "  - R0{wave}_AC1003\n",
      "  - R0{wave}R_A_CURRCIGUSE\n",
      "  - R0{wave}R_A_EVERCIGUSE\n"
     ]
    }
   ],
   "source": [
    "# Key variables to track across waves for smoking status\n",
    "# These patterns work for most waves (adjust if needed)\n",
    "SMOKING_STATUS_PATTERNS = [\n",
    "    'R0{wave}R_A_EVERSMOKE',      # Ever smoked\n",
    "    'R0{wave}_AC1002',             # Smoked in past 30 days\n",
    "    'R0{wave}_AC1003',             # Current smoking frequency (every day, some days, not at all)\n",
    "    'R0{wave}R_A_CURRCIGUSE',      # Current cigarette use (derived)\n",
    "    'R0{wave}R_A_EVERCIGUSE',      # Ever cigarette use (derived)\n",
    "]\n",
    "\n",
    "print(\"Variables to track smoking status transitions:\")\n",
    "for pattern in SMOKING_STATUS_PATTERNS:\n",
    "    print(f\"  - {pattern}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wave(wave_num, nrows=None):\n",
    "    \"\"\"\n",
    "    Load a single wave of PATH data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    wave_num : int\n",
    "        Wave number (1-5)\n",
    "    nrows : int, optional\n",
    "        Number of rows to load (for testing)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Wave data with wave number added as column\n",
    "    \"\"\"\n",
    "    path = WAVE_FILES[wave_num]\n",
    "    \n",
    "    if not path.exists():\n",
    "        print(f\"⚠️  Wave {wave_num} file not found: {path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading Wave {wave_num}...\", end=' ')\n",
    "    \n",
    "    # Load data - disable convert_categoricals to avoid duplicate label errors\n",
    "    reader = pd.read_stata(path, iterator=True, convert_categoricals=False)\n",
    "    df = reader.read(nrows=nrows)\n",
    "    \n",
    "    # Add wave identifier\n",
    "    df['wave'] = wave_num\n",
    "    \n",
    "    print(f\"✓ {len(df):,} rows, {len(df.columns):,} columns\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41cad0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading waves with sample_size=None...\n",
      "\n",
      "Loading Wave 1... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_69143/470547170.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wave'] = wave_num\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 32,320 rows, 1,743 columns\n",
      "Loading Wave 2... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_69143/470547170.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wave'] = wave_num\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 28,362 rows, 2,209 columns\n",
      "Loading Wave 3... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_69143/470547170.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wave'] = wave_num\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 28,148 rows, 2,141 columns\n",
      "Loading Wave 4... "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nValue labels for column R04_AN0336 are not unique. These cannot be converted to\npandas categoricals.\n\nEither read the file with `convert_categoricals` set to False or use the\nlow level interface in `StataReader` to separately read the values and the\nvalue_labels.\n\nThe repeated labels are:\n--------------------------------------------------------------------------------\nsmoke/use\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data mining/smoking_cessation_ml/.venv/lib/python3.13/site-packages/pandas/io/stata.py:1967\u001b[39m, in \u001b[36mStataReader._do_convert_categoricals\u001b[39m\u001b[34m(self, data, value_label_dict, lbllist, order_categoricals)\u001b[39m\n\u001b[32m   1964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1965\u001b[39m     \u001b[38;5;66;03m# Try to catch duplicate categories\u001b[39;00m\n\u001b[32m   1966\u001b[39m     \u001b[38;5;66;03m# TODO: if we get a non-copying rename_categories, use that\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1967\u001b[39m     cat_data = \u001b[43mcat_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrename_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1968\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data mining/smoking_cessation_ml/.venv/lib/python3.13/site-packages/pandas/core/arrays/categorical.py:1206\u001b[39m, in \u001b[36mCategorical.rename_categories\u001b[39m\u001b[34m(self, new_categories)\u001b[39m\n\u001b[32m   1205\u001b[39m cat = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m-> \u001b[39m\u001b[32m1206\u001b[39m \u001b[43mcat\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_set_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_categories\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data mining/smoking_cessation_ml/.venv/lib/python3.13/site-packages/pandas/core/arrays/categorical.py:925\u001b[39m, in \u001b[36mCategorical._set_categories\u001b[39m\u001b[34m(self, categories, fastpath)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     new_dtype = \u001b[43mCategoricalDtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mordered\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    927\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fastpath\n\u001b[32m    928\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dtype.categories \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    929\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_dtype.categories) != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.dtype.categories)\n\u001b[32m    930\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data mining/smoking_cessation_ml/.venv/lib/python3.13/site-packages/pandas/core/dtypes/dtypes.py:221\u001b[39m, in \u001b[36mCategoricalDtype.__init__\u001b[39m\u001b[34m(self, categories, ordered)\u001b[39m\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, categories=\u001b[38;5;28;01mNone\u001b[39;00m, ordered: Ordered = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_finalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mordered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data mining/smoking_cessation_ml/.venv/lib/python3.13/site-packages/pandas/core/dtypes/dtypes.py:378\u001b[39m, in \u001b[36mCategoricalDtype._finalize\u001b[39m\u001b[34m(self, categories, ordered, fastpath)\u001b[39m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m categories \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     categories = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfastpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28mself\u001b[39m._categories = categories\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data mining/smoking_cessation_ml/.venv/lib/python3.13/site-packages/pandas/core/dtypes/dtypes.py:579\u001b[39m, in \u001b[36mCategoricalDtype.validate_categories\u001b[39m\u001b[34m(categories, fastpath)\u001b[39m\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m categories.is_unique:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCategorical categories must be unique\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(categories, ABCCategoricalIndex):\n",
      "\u001b[31mValueError\u001b[39m: Categorical categories must be unique",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m waves_data = {}\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m wave_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m6\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     df = \u001b[43mload_wave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwave_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSAMPLE_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     13\u001b[39m         waves_data[wave_num] = df\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mload_wave\u001b[39m\u001b[34m(wave_num, nrows)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m     26\u001b[39m reader = pd.read_stata(path, iterator=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m df = \u001b[43mreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Add wave identifier\u001b[39;00m\n\u001b[32m     30\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mwave\u001b[39m\u001b[33m'\u001b[39m] = wave_num\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data mining/smoking_cessation_ml/.venv/lib/python3.13/site-packages/pandas/io/stata.py:1799\u001b[39m, in \u001b[36mStataReader.read\u001b[39m\u001b[34m(self, nrows, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals)\u001b[39m\n\u001b[32m   1794\u001b[39m             data.isetitem(\n\u001b[32m   1795\u001b[39m                 i, _stata_elapsed_date_to_datetime_vec(data.iloc[:, i], fmt)\n\u001b[32m   1796\u001b[39m             )\n\u001b[32m   1798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_categoricals \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_version > \u001b[32m108\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1799\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_convert_categoricals\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1800\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_value_label_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_lbllist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_categoricals\u001b[49m\n\u001b[32m   1801\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m preserve_dtypes:\n\u001b[32m   1804\u001b[39m     retyped_data = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/data mining/smoking_cessation_ml/.venv/lib/python3.13/site-packages/pandas/io/stata.py:1984\u001b[39m, in \u001b[36mStataReader._do_convert_categoricals\u001b[39m\u001b[34m(self, data, value_label_dict, lbllist, order_categoricals)\u001b[39m\n\u001b[32m   1972\u001b[39m                     \u001b[38;5;66;03m# GH 25772\u001b[39;00m\n\u001b[32m   1973\u001b[39m                     msg = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m   1974\u001b[39m \u001b[33mValue labels for column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m are not unique. These cannot be converted to\u001b[39m\n\u001b[32m   1975\u001b[39m \u001b[33mpandas categoricals.\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1982\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mrepeats\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   1983\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1984\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1985\u001b[39m                 \u001b[38;5;66;03m# TODO: is the next line needed above in the data(...) method?\u001b[39;00m\n\u001b[32m   1986\u001b[39m                 cat_series = Series(cat_data, index=data.index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: \nValue labels for column R04_AN0336 are not unique. These cannot be converted to\npandas categoricals.\n\nEither read the file with `convert_categoricals` set to False or use the\nlow level interface in `StataReader` to separately read the values and the\nvalue_labels.\n\nThe repeated labels are:\n--------------------------------------------------------------------------------\nsmoke/use\n"
     ]
    }
   ],
   "source": [
    "# Load all waves (start with sample for testing, then switch to full data)\n",
    "# For initial testing, use nrows=1000 per wave\n",
    "# For full run, use nrows=None\n",
    "\n",
    "SAMPLE_SIZE = None  # Set to None for full data, or 1000 for testing\n",
    "\n",
    "print(f\"Loading waves with sample_size={SAMPLE_SIZE}...\\n\")\n",
    "\n",
    "waves_data = {}\n",
    "for wave_num in range(1, 6):\n",
    "    df = load_wave(wave_num, nrows=SAMPLE_SIZE)\n",
    "    if df is not None:\n",
    "        waves_data[wave_num] = df\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(waves_data)} waves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c7602",
   "metadata": {},
   "source": [
    "## 3. Identify Baseline Smokers\n",
    "\n",
    "For each wave, identify current smokers who could potentially quit by the next wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_current_smokers(df, wave_num):\n",
    "    \"\"\"\n",
    "    Identify current smokers in a given wave.\n",
    "    \n",
    "    Current smoker definition:\n",
    "    - Smoked in past 30 days (R0X_AC1002 = 1 \"Yes\")\n",
    "    - OR smoking frequency is \"Every day\" or \"Some days\" (R0X_AC1003 = 1 or 2)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Wave data\n",
    "    wave_num : int\n",
    "        Wave number for variable names\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Boolean series indicating current smokers\n",
    "    \"\"\"\n",
    "    # Variable names for this wave\n",
    "    smoked_30d = f'R0{wave_num}_AC1002'  # Past 30 day smoking\n",
    "    freq_var = f'R0{wave_num}_AC1003'     # Smoking frequency\n",
    "    \n",
    "    # Extract numeric codes from categorical variables\n",
    "    from src.feature_engineering import _extract_numeric_code\n",
    "    \n",
    "    is_smoker = pd.Series(False, index=df.index)\n",
    "    \n",
    "    # Check if variables exist\n",
    "    if smoked_30d in df.columns:\n",
    "        smoked_code = _extract_numeric_code(df[smoked_30d])\n",
    "        is_smoker |= (smoked_code == 1)  # 1 = Yes\n",
    "    \n",
    "    if freq_var in df.columns:\n",
    "        freq_code = _extract_numeric_code(df[freq_var])\n",
    "        is_smoker |= (freq_code.isin([1, 2]))  # 1 = Every day, 2 = Some days\n",
    "    \n",
    "    return is_smoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32359607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count current smokers in each wave\n",
    "print(\"Current smokers by wave:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "smoker_counts = {}\n",
    "for wave_num, df in waves_data.items():\n",
    "    is_smoker = identify_current_smokers(df, wave_num)\n",
    "    n_smokers = is_smoker.sum()\n",
    "    pct_smokers = 100 * n_smokers / len(df)\n",
    "    \n",
    "    smoker_counts[wave_num] = n_smokers\n",
    "    print(f\"Wave {wave_num}: {n_smokers:>6,} / {len(df):>6,} ({pct_smokers:>5.1f}%)\")\n",
    "    \n",
    "    # Store flag in dataframe\n",
    "    waves_data[wave_num]['is_current_smoker'] = is_smoker\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e4388",
   "metadata": {},
   "source": [
    "## 4. Create Person-Period Transitions\n",
    "\n",
    "For each person who is a smoker at wave t, create a record with:\n",
    "- Baseline features from wave t\n",
    "- Outcome (quit_success) from wave t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c49627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transitions(wave_t_data, wave_t1_data, wave_t, wave_t1):\n",
    "    \"\"\"\n",
    "    Create transition records from wave t to wave t+1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    wave_t_data : pd.DataFrame\n",
    "        Baseline wave data\n",
    "    wave_t1_data : pd.DataFrame\n",
    "        Follow-up wave data\n",
    "    wave_t : int\n",
    "        Baseline wave number\n",
    "    wave_t1 : int\n",
    "        Follow-up wave number\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Transition records with baseline features and quit outcome\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating transitions: Wave {wave_t} → Wave {wave_t1}\")\n",
    "    \n",
    "    # Get smokers at baseline\n",
    "    smokers_t = wave_t_data[wave_t_data['is_current_smoker']].copy()\n",
    "    print(f\"  Baseline smokers: {len(smokers_t):,}\")\n",
    "    \n",
    "    # Merge with follow-up data on PERSONID\n",
    "    transitions = smokers_t.merge(\n",
    "        wave_t1_data[['PERSONID', 'is_current_smoker']],\n",
    "        on='PERSONID',\n",
    "        how='inner',\n",
    "        suffixes=('', '_t1')\n",
    "    )\n",
    "    \n",
    "    print(f\"  With follow-up data: {len(transitions):,}\")\n",
    "    \n",
    "    # Define quit success: was smoking at t, not smoking at t+1\n",
    "    transitions['quit_success'] = (~transitions['is_current_smoker_t1']).astype(int)\n",
    "    \n",
    "    # Add transition info\n",
    "    transitions['baseline_wave'] = wave_t\n",
    "    transitions['followup_wave'] = wave_t1\n",
    "    transitions['transition'] = f'W{wave_t}→W{wave_t1}'\n",
    "    \n",
    "    # Calculate quit rate\n",
    "    quit_rate = 100 * transitions['quit_success'].mean()\n",
    "    print(f\"  Quit rate: {quit_rate:.1f}%\")\n",
    "    \n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56dac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transitions for all consecutive wave pairs\n",
    "print(\"Creating person-period transitions...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_transitions = []\n",
    "\n",
    "for wave_t in range(1, 5):  # Waves 1-4 (need t+1 for outcome)\n",
    "    wave_t1 = wave_t + 1\n",
    "    \n",
    "    if wave_t in waves_data and wave_t1 in waves_data:\n",
    "        transitions = create_transitions(\n",
    "            waves_data[wave_t],\n",
    "            waves_data[wave_t1],\n",
    "            wave_t,\n",
    "            wave_t1\n",
    "        )\n",
    "        all_transitions.append(transitions)\n",
    "\n",
    "# Pool all transitions\n",
    "if all_transitions:\n",
    "    pooled = pd.concat(all_transitions, ignore_index=True)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"✓ Total transitions: {len(pooled):,}\")\n",
    "    print(f\"✓ Overall quit rate: {100 * pooled['quit_success'].mean():.1f}%\")\n",
    "    print(f\"✓ Unique persons: {pooled['PERSONID'].nunique():,}\")\n",
    "else:\n",
    "    print(\"⚠️  No transitions created\")\n",
    "    pooled = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3a128",
   "metadata": {},
   "source": [
    "## 5. Apply Feature Engineering\n",
    "\n",
    "Use Phase 3 feature engineering pipeline on the pooled transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codebook overrides from Phase 3\n",
    "codebook_overrides = {\n",
    "    # Demographics\n",
    "    'age': 'R01R_A_AGECAT7',  # Use wave-specific version (adjust for each wave)\n",
    "    'sex': 'R01R_A_SEX',\n",
    "    'income': 'R01R_POVCAT3',\n",
    "    'education_code': None,  # Not available\n",
    "    \n",
    "    # Race/Ethnicity\n",
    "    'race': 'R01R_A_RACECAT3',\n",
    "    'hispanic': 'R01R_A_HISP',\n",
    "    'race_map': {1: 'White', 2: 'Black', 3: 'Other'},\n",
    "    'hisp_yes_values': (1,),\n",
    "    'race_collapse_to_other': (),\n",
    "    \n",
    "    # Smoking behavior\n",
    "    'cpd': 'R01R_A_PERDAY_P30D_CIGS',\n",
    "    'ttfc_minutes': 'R01R_A_MINFIRST_CIGS',\n",
    "    \n",
    "    # Cessation methods\n",
    "    'nrt_any': 'R01R_A_PST12M_LSTQUIT_NRT',\n",
    "    'varenicline': 'R01R_A_PST12M_LSTQUIT_RX',\n",
    "}\n",
    "\n",
    "print(\"Codebook overrides configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b609aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_overrides_for_wave(overrides, wave_num):\n",
    "    \"\"\"\n",
    "    Adjust variable names for specific wave.\n",
    "    Replace R01R_ prefix with R0{wave}R_.\n",
    "    \"\"\"\n",
    "    adjusted = {}\n",
    "    for key, value in overrides.items():\n",
    "        if isinstance(value, str) and value.startswith('R01R_'):\n",
    "            adjusted[key] = value.replace('R01R_', f'R0{wave_num}R_')\n",
    "        elif isinstance(value, str) and value.startswith('R01_'):\n",
    "            adjusted[key] = value.replace('R01_', f'R0{wave_num}_')\n",
    "        else:\n",
    "            adjusted[key] = value\n",
    "    return adjusted\n",
    "\n",
    "# Test\n",
    "wave1_overrides = adjust_overrides_for_wave(codebook_overrides, 1)\n",
    "wave2_overrides = adjust_overrides_for_wave(codebook_overrides, 2)\n",
    "\n",
    "print(\"Wave-specific variable adjustment function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0959a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pooled is not None:\n",
    "    print(\"Applying feature engineering to pooled transitions...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # We need to handle multiple waves - for now, use the baseline_wave for variable names\n",
    "    # More sophisticated approach: engineer features separately per wave then pool\n",
    "    \n",
    "    # Simple approach: most transitions are W1→W2, use W1 variables\n",
    "    # For production, you'd want to engineer features per transition\n",
    "    \n",
    "    print(\"Transition distribution:\")\n",
    "    print(pooled['transition'].value_counts())\n",
    "    print()\n",
    "    \n",
    "    # Engineer features (assuming W1 variable names dominate)\n",
    "    wave1_overrides = adjust_overrides_for_wave(codebook_overrides, 1)\n",
    "    \n",
    "    print(\"Running feature engineering...\")\n",
    "    engineered = engineer_all_features(\n",
    "        pooled.copy(),\n",
    "        codebook_overrides=wave1_overrides,\n",
    "        recode_missing=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Features created: {engineered.shape[1]} columns\")\n",
    "    print(f\"✓ Records: {len(engineered):,}\")\n",
    "else:\n",
    "    print(\"⚠️  No data to engineer features\")\n",
    "    engineered = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d30a6",
   "metadata": {},
   "source": [
    "## 6. Validate and Summarize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b474419",
   "metadata": {},
   "outputs": [],
   "source": [
    "if engineered is not None:\n",
    "    print(\"Data Quality Summary\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall shape\n",
    "    print(f\"\\nDataset shape: {engineered.shape[0]:,} rows × {engineered.shape[1]:,} columns\")\n",
    "    \n",
    "    # Outcome distribution\n",
    "    print(f\"\\nOutcome (quit_success):\")\n",
    "    print(engineered['quit_success'].value_counts())\n",
    "    print(f\"Quit rate: {100 * engineered['quit_success'].mean():.1f}%\")\n",
    "    \n",
    "    # Missing data in key features\n",
    "    from src.feature_engineering import get_feature_list\n",
    "    feature_cols = get_feature_list()\n",
    "    available_features = [f for f in feature_cols if f in engineered.columns]\n",
    "    \n",
    "    print(f\"\\nFeature availability: {len(available_features)}/{len(feature_cols)}\")\n",
    "    \n",
    "    # Check missing rates for key features\n",
    "    print(\"\\nMissing rates for key features:\")\n",
    "    key_features = ['age', 'sex', 'cpd', 'ttfc_minutes', 'high_dependence', \n",
    "                    'race_white', 'used_nrt', 'used_varenicline']\n",
    "    \n",
    "    for feat in key_features:\n",
    "        if feat in engineered.columns:\n",
    "            missing_pct = 100 * engineered[feat].isna().mean()\n",
    "            print(f\"  {feat:20s}: {missing_pct:>5.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c889db1",
   "metadata": {},
   "source": [
    "## 7. Save Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5318a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if engineered is not None:\n",
    "    # Select features for modeling\n",
    "    from src.feature_engineering import get_feature_list\n",
    "    \n",
    "    feature_cols = get_feature_list()\n",
    "    available_features = [f for f in feature_cols if f in engineered.columns]\n",
    "    \n",
    "    # Add ID and outcome columns\n",
    "    modeling_cols = ['PERSONID', 'baseline_wave', 'followup_wave', 'transition', 'quit_success'] + available_features\n",
    "    existing_cols = [c for c in modeling_cols if c in engineered.columns]\n",
    "    \n",
    "    modeling_data = engineered[existing_cols].copy()\n",
    "    \n",
    "    # Save as CSV and parquet\n",
    "    csv_path = OUTPUT_DIR / 'pooled_transitions.csv'\n",
    "    parquet_path = OUTPUT_DIR / 'pooled_transitions.parquet'\n",
    "    \n",
    "    print(\"Saving dataset...\")\n",
    "    modeling_data.to_csv(csv_path, index=False)\n",
    "    modeling_data.to_parquet(parquet_path, index=False)\n",
    "    \n",
    "    print(f\"\\n✓ Saved: {csv_path}\")\n",
    "    print(f\"✓ Saved: {parquet_path}\")\n",
    "    print(f\"\\nFinal dataset: {len(modeling_data):,} rows × {len(modeling_data.columns):,} columns\")\n",
    "else:\n",
    "    print(\"⚠️  No data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258d6938",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. ✅ Loaded all 5 waves of PATH Adult Public Use Files\n",
    "2. ✅ Identified current smokers in each wave using standardized criteria\n",
    "3. ✅ Created person-period transitions from wave t to wave t+1\n",
    "4. ✅ Defined quit outcome: smoking at t → not smoking at t+1\n",
    "5. ✅ Applied feature engineering from Phase 3\n",
    "6. ✅ Validated data quality and feature availability\n",
    "7. ✅ Saved `pooled_transitions.csv` for modeling\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- Total transitions created from consecutive waves\n",
    "- Overall quit rate across all transitions\n",
    "- Feature availability and missing data patterns\n",
    "- Ready for modeling in Phase 4\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Re-run modeling pipeline** with real data and outcomes\n",
    "2. **Analyze feature importance** to identify key predictors\n",
    "3. **Search for additional features** (quit history, motivation)\n",
    "4. **Hyperparameter tuning** for better performance\n",
    "5. **Final evaluation** on test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
