{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eabdda0",
   "metadata": {},
   "source": [
    "# Phase 2: Data Preprocessing & Longitudinal Transitions\n",
    "\n",
    "Process full PATH Study data (Waves 1-5) to create person-period dataset with quit outcomes.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Load all 7 waves of PATH Adult Public Use Files\n",
    "2. Create longitudinal person-period structure\n",
    "3. Define quit outcome: smoking status at wave t+1\n",
    "4. Filter to baseline smokers with follow-up data\n",
    "5. Apply feature engineering from Phase 3 (motivation + environment updates)\n",
    "6. Normalize PATH negative missing codes (e.g. -9, -8, -1 plus extended set)\n",
    "7. Save compact dataset with canonical engineered features only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee533b25",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e69fd5e",
   "metadata": {},
   "source": [
    "## Optional: Run Full Preprocessing Script\n",
    "\n",
    "Run the standalone pipeline to regenerate the processed dataset with only canonical engineered features (no raw or alias columns):\n",
    "\n",
    "```python\n",
    "!python ../scripts/run_preprocessing.py\n",
    "```\n",
    "\n",
    "Reload and inspect basic shape:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "processed = pd.read_csv('../data/processed/pooled_transitions.csv')\n",
    "print('Rows, Cols:', processed.shape)\n",
    "print('Columns (first 25):', list(processed.columns[:25]))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c480705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "# Import feature engineering\n",
    "from src.feature_engineering import engineer_all_features, map_from_codebook\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47eafa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for PATH data files:\n",
      "  ✓ Wave 1: PATH_W1_Adult_Public.dta\n",
      "  ✓ Wave 2: PATH_W2_Adult_Public.dta\n",
      "  ✓ Wave 3: PATH_W3_Adult_Public.dta\n",
      "  ✓ Wave 4: PATH_W4_Adult_Public.dta\n",
      "  ✓ Wave 5: PATH_W5_Adult_Public.dta\n",
      "  ✓ Wave 6: PATH_W6_Adult_Public.dta\n",
      "  ✓ Wave 7: PATH_W7_Adult_Public.dta\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = Path('../data/raw')\n",
    "OUTPUT_DIR = Path('../data/processed')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Wave files\n",
    "WAVE_FILES = {\n",
    "    1: DATA_DIR / 'PATH_W1_Adult_Public.dta',\n",
    "    2: DATA_DIR / 'PATH_W2_Adult_Public.dta',\n",
    "    3: DATA_DIR / 'PATH_W3_Adult_Public.dta',\n",
    "    4: DATA_DIR / 'PATH_W4_Adult_Public.dta',\n",
    "    5: DATA_DIR / 'PATH_W5_Adult_Public.dta',\n",
    "    6: DATA_DIR / 'PATH_W6_Adult_Public.dta',\n",
    "    7: DATA_DIR / 'PATH_W7_Adult_Public.dta'\n",
    "}\n",
    "\n",
    "# Check which files exist\n",
    "print(\"Checking for PATH data files:\")\n",
    "for wave, path in WAVE_FILES.items():\n",
    "    status = \"✓\" if path.exists() else \"✗\"\n",
    "    print(f\"  {status} Wave {wave}: {path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d492cf",
   "metadata": {},
   "source": [
    "## 2. Load Individual Waves\n",
    "\n",
    "Load each wave and extract key smoking status variables for transition analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8426e454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables to track smoking status transitions:\n",
      "  - R0{wave}R_A_EVERSMOKE\n",
      "  - R0{wave}_AC1002\n",
      "  - R0{wave}_AC1003\n",
      "  - R0{wave}R_A_CURRCIGUSE\n",
      "  - R0{wave}R_A_EVERCIGUSE\n"
     ]
    }
   ],
   "source": [
    "# Key variables to track across waves for smoking status\n",
    "# These patterns work for most waves (adjust if needed)\n",
    "SMOKING_STATUS_PATTERNS = [\n",
    "    'R0{wave}R_A_EVERSMOKE',      # Ever smoked\n",
    "    'R0{wave}_AC1002',             # Smoked in past 30 days\n",
    "    'R0{wave}_AC1003',             # Current smoking frequency (every day, some days, not at all)\n",
    "    'R0{wave}R_A_CURRCIGUSE',      # Current cigarette use (derived)\n",
    "    'R0{wave}R_A_EVERCIGUSE',      # Ever cigarette use (derived)\n",
    "]\n",
    "\n",
    "print(\"Variables to track smoking status transitions:\")\n",
    "for pattern in SMOKING_STATUS_PATTERNS:\n",
    "    print(f\"  - {pattern}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc2b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wave(wave_num, nrows=None):\n",
    "    \"\"\"\n",
    "    Load a single wave of PATH data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    wave_num : int\n",
    "        Wave number (1-5)\n",
    "    nrows : int, optional\n",
    "        Number of rows to load (for testing)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Wave data with wave number added as column\n",
    "    \"\"\"\n",
    "    path = WAVE_FILES[wave_num]\n",
    "    \n",
    "    if not path.exists():\n",
    "        print(f\"⚠️  Wave {wave_num} file not found: {path}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading Wave {wave_num}...\", end=' ')\n",
    "    \n",
    "    # Load data - disable convert_categoricals to avoid duplicate label errors\n",
    "    reader = pd.read_stata(path, iterator=True, convert_categoricals=False)\n",
    "    df = reader.read(nrows=nrows)\n",
    "    \n",
    "    # Add wave identifier\n",
    "    df['wave'] = wave_num\n",
    "    \n",
    "    print(f\"✓ {len(df):,} rows, {len(df.columns):,} columns\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41cad0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading waves with sample_size=None...\n",
      "\n",
      "Loading Wave 1... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/573832500.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wave'] = wave_num\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 32,320 rows, 1,743 columns\n",
      "Loading Wave 2... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/573832500.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wave'] = wave_num\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 28,362 rows, 2,209 columns\n",
      "Loading Wave 3... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/573832500.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wave'] = wave_num\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 28,148 rows, 2,141 columns\n",
      "Loading Wave 4... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/573832500.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wave'] = wave_num\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 33,822 rows, 2,182 columns\n",
      "Loading Wave 5... ✓ 34,309 rows, 2,316 columns\n",
      "\n",
      "✓ Loaded 5 waves\n",
      "✓ 34,309 rows, 2,316 columns\n",
      "\n",
      "✓ Loaded 5 waves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/573832500.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['wave'] = wave_num\n"
     ]
    }
   ],
   "source": [
    "# Load all waves (start with sample for testing, then switch to full data)\n",
    "# For initial testing, use nrows=1000 per wave\n",
    "# For full run, use nrows=None\n",
    "\n",
    "SAMPLE_SIZE = None  # Set to None for full data, or 1000 for testing\n",
    "\n",
    "print(f\"Loading waves with sample_size={SAMPLE_SIZE}...\\n\")\n",
    "\n",
    "waves_data = {}\n",
    "for wave_num in range(1, 6):\n",
    "    df = load_wave(wave_num, nrows=SAMPLE_SIZE)\n",
    "    if df is not None:\n",
    "        waves_data[wave_num] = df\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(waves_data)} waves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c7602",
   "metadata": {},
   "source": [
    "## 3. Identify Baseline Smokers\n",
    "\n",
    "For each wave, identify current smokers who could potentially quit by the next wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7a8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_current_smokers(df, wave_num):\n",
    "    \"\"\"\n",
    "    Identify current smokers in a given wave.\n",
    "    \n",
    "    Current smoker definition:\n",
    "    - Smoked in past 30 days (R0X_AC1002 = 1 \"Yes\")\n",
    "    - OR smoking frequency is \"Every day\" or \"Some days\" (R0X_AC1003 = 1 or 2)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Wave data\n",
    "    wave_num : int\n",
    "        Wave number for variable names\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Boolean series indicating current smokers\n",
    "    \"\"\"\n",
    "    # Variable names for this wave\n",
    "    smoked_30d = f'R0{wave_num}_AC1002'  # Past 30 day smoking\n",
    "    freq_var = f'R0{wave_num}_AC1003'     # Smoking frequency\n",
    "    \n",
    "    # Extract numeric codes from categorical variables\n",
    "    from src.feature_engineering import _extract_numeric_code\n",
    "    \n",
    "    is_smoker = pd.Series(False, index=df.index)\n",
    "    \n",
    "    # Check if variables exist\n",
    "    if smoked_30d in df.columns:\n",
    "        smoked_code = _extract_numeric_code(df[smoked_30d])\n",
    "        is_smoker |= (smoked_code == 1)  # 1 = Yes\n",
    "    \n",
    "    if freq_var in df.columns:\n",
    "        freq_code = _extract_numeric_code(df[freq_var])\n",
    "        is_smoker |= (freq_code.isin([1, 2]))  # 1 = Every day, 2 = Some days\n",
    "    \n",
    "    return is_smoker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32359607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/2543311249.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  waves_data[wave_num]['is_current_smoker'] = is_smoker\n",
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/2543311249.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  waves_data[wave_num]['is_current_smoker'] = is_smoker\n",
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/2543311249.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  waves_data[wave_num]['is_current_smoker'] = is_smoker\n",
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/2543311249.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  waves_data[wave_num]['is_current_smoker'] = is_smoker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current smokers by wave:\n",
      "==================================================\n",
      "Wave 1: 25,183 / 32,320 ( 77.9%)\n",
      "Wave 2: 10,722 / 28,362 ( 37.8%)\n",
      "Wave 3:  9,817 / 28,148 ( 34.9%)\n",
      "Wave 4: 10,967 / 33,822 ( 32.4%)\n",
      "Wave 5:  9,705 / 34,309 ( 28.3%)\n",
      "==================================================\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_8/xdjlqs6x4531r8qcsc4wzfb40000gn/T/ipykernel_59448/2543311249.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  waves_data[wave_num]['is_current_smoker'] = is_smoker\n"
     ]
    }
   ],
   "source": [
    "# Count current smokers in each wave\n",
    "print(\"Current smokers by wave:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "smoker_counts = {}\n",
    "for wave_num, df in waves_data.items():\n",
    "    is_smoker = identify_current_smokers(df, wave_num)\n",
    "    n_smokers = is_smoker.sum()\n",
    "    pct_smokers = 100 * n_smokers / len(df)\n",
    "    \n",
    "    smoker_counts[wave_num] = n_smokers\n",
    "    print(f\"Wave {wave_num}: {n_smokers:>6,} / {len(df):>6,} ({pct_smokers:>5.1f}%)\")\n",
    "    \n",
    "    # Store flag in dataframe\n",
    "    waves_data[wave_num]['is_current_smoker'] = is_smoker\n",
    "\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5e4388",
   "metadata": {},
   "source": [
    "## 4. Create Person-Period Transitions\n",
    "\n",
    "For each person who is a smoker at wave t, create a record with:\n",
    "- Baseline features from wave t\n",
    "- Outcome (quit_success) from wave t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05c49627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transitions(wave_t_data, wave_t1_data, wave_t, wave_t1):\n",
    "    \"\"\"\n",
    "    Create transition records from wave t to wave t+1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    wave_t_data : pd.DataFrame\n",
    "        Baseline wave data\n",
    "    wave_t1_data : pd.DataFrame\n",
    "        Follow-up wave data\n",
    "    wave_t : int\n",
    "        Baseline wave number\n",
    "    wave_t1 : int\n",
    "        Follow-up wave number\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Transition records with baseline features and quit outcome\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating transitions: Wave {wave_t} → Wave {wave_t1}\")\n",
    "    \n",
    "    # Get smokers at baseline\n",
    "    smokers_t = wave_t_data[wave_t_data['is_current_smoker']].copy()\n",
    "    print(f\"  Baseline smokers: {len(smokers_t):,}\")\n",
    "    \n",
    "    # Merge with follow-up data on PERSONID\n",
    "    transitions = smokers_t.merge(\n",
    "        wave_t1_data[['PERSONID', 'is_current_smoker']],\n",
    "        on='PERSONID',\n",
    "        how='inner',\n",
    "        suffixes=('', '_t1')\n",
    "    )\n",
    "    \n",
    "    print(f\"  With follow-up data: {len(transitions):,}\")\n",
    "    \n",
    "    # Define quit success: was smoking at t, not smoking at t+1\n",
    "    transitions['quit_success'] = (~transitions['is_current_smoker_t1']).astype(int)\n",
    "    \n",
    "    # Add transition info\n",
    "    transitions['baseline_wave'] = wave_t\n",
    "    transitions['followup_wave'] = wave_t1\n",
    "    transitions['transition'] = f'W{wave_t}→W{wave_t1}'\n",
    "    \n",
    "    # Calculate quit rate\n",
    "    quit_rate = 100 * transitions['quit_success'].mean()\n",
    "    print(f\"  Quit rate: {quit_rate:.1f}%\")\n",
    "    \n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc56dac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating person-period transitions...\n",
      "======================================================================\n",
      "\n",
      "Creating transitions: Wave 1 → Wave 2\n",
      "  Baseline smokers: 25,183\n",
      "  Baseline smokers: 25,183\n",
      "  With follow-up data: 20,656\n",
      "  Quit rate: 50.2%\n",
      "\n",
      "Creating transitions: Wave 2 → Wave 3\n",
      "  With follow-up data: 20,656\n",
      "  Quit rate: 50.2%\n",
      "\n",
      "Creating transitions: Wave 2 → Wave 3\n",
      "  Baseline smokers: 10,722\n",
      "  With follow-up data: 9,504\n",
      "  Quit rate: 12.6%\n",
      "\n",
      "Creating transitions: Wave 3 → Wave 4\n",
      "  Baseline smokers: 10,722\n",
      "  With follow-up data: 9,504\n",
      "  Quit rate: 12.6%\n",
      "\n",
      "Creating transitions: Wave 3 → Wave 4\n",
      "  Baseline smokers: 9,817\n",
      "  With follow-up data: 8,618\n",
      "  Quit rate: 12.2%\n",
      "\n",
      "Creating transitions: Wave 4 → Wave 5\n",
      "  Baseline smokers: 9,817\n",
      "  With follow-up data: 8,618\n",
      "  Quit rate: 12.2%\n",
      "\n",
      "Creating transitions: Wave 4 → Wave 5\n",
      "  Baseline smokers: 10,967\n",
      "  With follow-up data: 9,104\n",
      "  Quit rate: 17.7%\n",
      "  Baseline smokers: 10,967\n",
      "  With follow-up data: 9,104\n",
      "  Quit rate: 17.7%\n",
      "\n",
      "======================================================================\n",
      "✓ Total transitions: 47,882\n",
      "✓ Overall quit rate: 29.7%\n",
      "✓ Unique persons: 23,411\n",
      "\n",
      "======================================================================\n",
      "✓ Total transitions: 47,882\n",
      "✓ Overall quit rate: 29.7%\n",
      "✓ Unique persons: 23,411\n"
     ]
    }
   ],
   "source": [
    "# Create transitions for all consecutive wave pairs\n",
    "print(\"Creating person-period transitions...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "all_transitions = []\n",
    "\n",
    "for wave_t in range(1, 5):  # Waves 1-4 (need t+1 for outcome)\n",
    "    wave_t1 = wave_t + 1\n",
    "    \n",
    "    if wave_t in waves_data and wave_t1 in waves_data:\n",
    "        transitions = create_transitions(\n",
    "            waves_data[wave_t],\n",
    "            waves_data[wave_t1],\n",
    "            wave_t,\n",
    "            wave_t1\n",
    "        )\n",
    "        all_transitions.append(transitions)\n",
    "\n",
    "# Pool all transitions\n",
    "if all_transitions:\n",
    "    pooled = pd.concat(all_transitions, ignore_index=True)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"✓ Total transitions: {len(pooled):,}\")\n",
    "    print(f\"✓ Overall quit rate: {100 * pooled['quit_success'].mean():.1f}%\")\n",
    "    print(f\"✓ Unique persons: {pooled['PERSONID'].nunique():,}\")\n",
    "else:\n",
    "    print(\"⚠️  No transitions created\")\n",
    "    pooled = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c3a128",
   "metadata": {},
   "source": [
    "## 5. Apply Feature Engineering\n",
    "\n",
    "Use Phase 3 feature engineering pipeline on the pooled transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee8c3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codebook overrides configured\n"
     ]
    }
   ],
   "source": [
    "# Codebook overrides from Phase 3\n",
    "codebook_overrides = {\n",
    "    # Demographics\n",
    "    'age': 'R01R_A_AGECAT7',  # Use wave-specific version (adjust for each wave)\n",
    "    'sex': 'R01R_A_SEX',\n",
    "    'income': 'R01R_POVCAT3',\n",
    "    'education_code': None,  # Not available\n",
    "    \n",
    "    # Race/Ethnicity\n",
    "    'race': 'R01R_A_RACECAT3',\n",
    "    'hispanic': 'R01R_A_HISP',\n",
    "    'race_map': {1: 'White', 2: 'Black', 3: 'Other'},\n",
    "    'hisp_yes_values': (1,),\n",
    "    'race_collapse_to_other': (),\n",
    "    \n",
    "    # Smoking behavior\n",
    "    'cpd': 'R01R_A_PERDAY_P30D_CIGS',\n",
    "    'ttfc_minutes': 'R01R_A_MINFIRST_CIGS',\n",
    "    \n",
    "    # Cessation methods\n",
    "    'nrt_any': 'R01R_A_PST12M_LSTQUIT_NRT',\n",
    "    'varenicline': 'R01R_A_PST12M_LSTQUIT_RX',\n",
    "}\n",
    "\n",
    "print(\"Codebook overrides configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b609aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wave-specific variable adjustment function ready\n"
     ]
    }
   ],
   "source": [
    "def adjust_overrides_for_wave(overrides, wave_num):\n",
    "    \"\"\"\n",
    "    Adjust variable names for specific wave.\n",
    "    Replace R01R_ prefix with R0{wave}R_.\n",
    "    \"\"\"\n",
    "    adjusted = {}\n",
    "    for key, value in overrides.items():\n",
    "        if isinstance(value, str) and value.startswith('R01R_'):\n",
    "            adjusted[key] = value.replace('R01R_', f'R0{wave_num}R_')\n",
    "        elif isinstance(value, str) and value.startswith('R01_'):\n",
    "            adjusted[key] = value.replace('R01_', f'R0{wave_num}_')\n",
    "        else:\n",
    "            adjusted[key] = value\n",
    "    return adjusted\n",
    "\n",
    "# Test\n",
    "wave1_overrides = adjust_overrides_for_wave(codebook_overrides, 1)\n",
    "wave2_overrides = adjust_overrides_for_wave(codebook_overrides, 2)\n",
    "\n",
    "print(\"Wave-specific variable adjustment function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0959a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying feature engineering to pooled transitions...\n",
      "======================================================================\n",
      "Transition distribution:\n",
      "transition\n",
      "W1→W2    20656\n",
      "W2→W3     9504\n",
      "W4→W5     9104\n",
      "W3→W4     8618\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Running feature engineering...\n",
      "\n",
      "✓ Features created: 8341 columns\n",
      "✓ Records: 47,882\n",
      "\n",
      "✓ Features created: 8341 columns\n",
      "✓ Records: 47,882\n"
     ]
    }
   ],
   "source": [
    "if pooled is not None:\n",
    "    print(\"Applying feature engineering to pooled transitions...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # We need to handle multiple waves - for now, use the baseline_wave for variable names\n",
    "    # More sophisticated approach: engineer features separately per wave then pool\n",
    "    \n",
    "    # Simple approach: most transitions are W1→W2, use W1 variables\n",
    "    # For production, you'd want to engineer features per transition\n",
    "    \n",
    "    print(\"Transition distribution:\")\n",
    "    print(pooled['transition'].value_counts())\n",
    "    print()\n",
    "    \n",
    "    # Engineer features (assuming W1 variable names dominate)\n",
    "    wave1_overrides = adjust_overrides_for_wave(codebook_overrides, 1)\n",
    "    \n",
    "    print(\"Running feature engineering...\")\n",
    "    engineered = engineer_all_features(\n",
    "        pooled.copy(),\n",
    "        codebook_overrides=wave1_overrides,\n",
    "        recode_missing=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✓ Features created: {engineered.shape[1]} columns\")\n",
    "    print(f\"✓ Records: {len(engineered):,}\")\n",
    "else:\n",
    "    print(\"⚠️  No data to engineer features\")\n",
    "    engineered = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d30a6",
   "metadata": {},
   "source": [
    "## 6. Validate and Summarize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b474419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Summary\n",
      "======================================================================\n",
      "\n",
      "Dataset shape: 47,882 rows × 8,341 columns\n",
      "\n",
      "Outcome (quit_success):\n",
      "quit_success\n",
      "0    33662\n",
      "1    14220\n",
      "Name: count, dtype: int64\n",
      "Quit rate: 29.7%\n",
      "\n",
      "Feature availability: 52/52\n",
      "\n",
      "Missing rates for key features:\n",
      "  age                 :   0.0% missing\n",
      "  sex                 :   0.1% missing\n",
      "  cpd                 :  77.7% missing\n",
      "  ttfc_minutes        :  26.4% missing\n",
      "  high_dependence     :   0.0% missing\n",
      "  race_white          :   0.0% missing\n",
      "  used_nrt            :   0.0% missing\n",
      "  used_varenicline    :   0.0% missing\n"
     ]
    }
   ],
   "source": [
    "if engineered is not None:\n",
    "    print(\"Data Quality Summary\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Overall shape\n",
    "    print(f\"\\nDataset shape: {engineered.shape[0]:,} rows × {engineered.shape[1]:,} columns\")\n",
    "    \n",
    "    # Outcome distribution\n",
    "    print(f\"\\nOutcome (quit_success):\")\n",
    "    print(engineered['quit_success'].value_counts())\n",
    "    print(f\"Quit rate: {100 * engineered['quit_success'].mean():.1f}%\")\n",
    "    \n",
    "    # Missing data in key features\n",
    "    from src.feature_engineering import get_feature_list\n",
    "    feature_cols = get_feature_list()\n",
    "    available_features = [f for f in feature_cols if f in engineered.columns]\n",
    "    \n",
    "    print(f\"\\nFeature availability: {len(available_features)}/{len(feature_cols)}\")\n",
    "    \n",
    "    # Check missing rates for key features\n",
    "    print(\"\\nMissing rates for key features:\")\n",
    "    key_features = ['age', 'sex', 'cpd', 'ttfc_minutes', 'high_dependence', \n",
    "                    'race_white', 'used_nrt', 'used_varenicline']\n",
    "    \n",
    "    for feat in key_features:\n",
    "        if feat in engineered.columns:\n",
    "            missing_pct = 100 * engineered[feat].isna().mean()\n",
    "            print(f\"  {feat:20s}: {missing_pct:>5.1f}% missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c889db1",
   "metadata": {},
   "source": [
    "## 7. Save Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5318a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving compact dataset with canonical features only...\n",
      "\n",
      "✓ Saved: ../data/processed/pooled_transitions.csv\n",
      "✓ Saved: ../data/processed/pooled_transitions.parquet\n",
      "Final dataset: 47,882 rows × 57 columns\n",
      "\n",
      "✓ Saved: ../data/processed/pooled_transitions.csv\n",
      "✓ Saved: ../data/processed/pooled_transitions.parquet\n",
      "Final dataset: 47,882 rows × 57 columns\n"
     ]
    }
   ],
   "source": [
    "if engineered is not None:\n",
    "    from src.feature_engineering import get_feature_list\n",
    "    feature_cols = get_feature_list()\n",
    "\n",
    "    # Ensure stable schema: add any missing engineered feature columns with zeros\n",
    "    missing_features = [f for f in feature_cols if f not in engineered.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Adding {len(missing_features)} missing feature column(s) with zeros:\")\n",
    "        for col in missing_features:\n",
    "            engineered[col] = 0\n",
    "            print(f\"  • {col}\")\n",
    "\n",
    "    # Only keep identifiers + canonical engineered features (no raw or alias columns)\n",
    "    modeling_cols = ['PERSONID', 'baseline_wave', 'followup_wave', 'transition', 'quit_success'] + feature_cols\n",
    "    existing_cols = [c for c in modeling_cols if c in engineered.columns]\n",
    "    modeling_data = engineered[existing_cols].copy()\n",
    "\n",
    "    csv_path = OUTPUT_DIR / 'pooled_transitions.csv'\n",
    "    parquet_path = OUTPUT_DIR / 'pooled_transitions.parquet'\n",
    "\n",
    "    print(\"Saving compact dataset with canonical features only...\")\n",
    "    modeling_data.to_csv(csv_path, index=False)\n",
    "    modeling_data.to_parquet(parquet_path, index=False)\n",
    "\n",
    "    print(f\"\\n✓ Saved: {csv_path}\")\n",
    "    print(f\"✓ Saved: {parquet_path}\")\n",
    "    print(f\"Final dataset: {len(modeling_data):,} rows × {len(modeling_data.columns):,} columns\")\n",
    "else:\n",
    "    print(\"⚠️  No data to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
